---
title: Classification de toute allure!
subtitle: Une analyse de la relation entre l’étiquette et le cheminement de circuits dans Trackmania Nations Forever utilisant la classification
highlight-style: github
authors:
  - name: Nicolas Kmita
    affiliation: École secondaire Franco-Cité
bibliography:
  - references.bib
  - packages.bib
nocite: |
  @*
notebook-links: false
lang: fr
format:
  titlepage-pdf:
    pdfengine: xelatex
    include-in-header:
      - text: |
          \usepackage{xcolor}
          \usepackage{lipsum}
      - macros.tex
    documentclass: scrreprt
    classoption: ["oneside", "open=any"]
    number-sections: true
    toc: true
    toc-title: "Table des matières"
    lof: false
    lot: false
    titlepage: classic-lined 
    titlepage-logo: "img/car-jump.png"
    titlepage-theme:
      elements: ["\\titleblock", "\\authorblock", "\\logoblock", "\\footerblock"]
      page-align: "center"
      title-style: "doublelinewide"
      title-fontsize: 30
      title-fontstyle: "uppercase"
      title-space-after: "0.1\\textheight"
      subtitle-fontstyle: ["Large", "textit"]
      author-style: "plain"
      author-sep: "\\hskip1em"
      author-fontstyle: "Large"
      author-space-after: "2\\baselineskip"
      affiliation-style: "numbered-list-with-correspondence"
      affiliation-fontstyle: "large"
      affiliation-space-after: "0pt"
      footer-style: "plain"
      footer-fontstyle: ["large", "textsc"]
      footer-space-after: "0pt"
      logo-size: "0.7\\textheight"
      logo-space-after: "1cm"
    titlepage-footer: |
      M. Chabot\
      MDM4U\
      25 octobre 2024\
    keep-tex: true
    geometry:
      - showframe
      - inner=2cm
      - outer=2cm
      - top=3cm
      - bottom=4cm
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
      - heightrounded
    indent: false
---

```

```

```{r}

# Préparation de librairies et installations variés

cran_mirror <- "https://mirror.csclub.uwaterloo.ca/CRAN/"

pkgs <- c(
  "tidyverse",
  "ggbeeswarm", # Génère des graphique d'essaim d'abeille
  "viridis", # Couleurs pouvant être mieux perçus
  "kableExtra",
  "rmarkdown",
  "knitr", # Tableaux
  "tinytex",
  "reshape",
  "ggh4x",
  "reticulate"
)

# Charger les « packages » et, s'ils ne sont pas installés, les installer du mirroir de CRAN indiqué ci-haut (UWaterloo par défaut).
for(pkg in pkgs) {
  # Voir https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them

  if(require(pkg, character.only=TRUE)){
    next
  }

  install.packages(pkg, repos=cran_mirror)
  library(pkg, character.only=TRUE)
}
# update.packages(ask = FALSE, repos=cran_mirror)

# Assurer la présence de TinyTeX
if (nchar(tinytex_root()) <= 0) {
  install_tinytex()
}

# Génération de citations
knitr::write_bib(pkgs, prefix="", file="packages.bib", tweak=FALSE)
```

```{r}
# Assurer l'utilisation des virgules par les sorties

inline_hook_old <- knit_hooks$get("inline")

format_d <- function(x) {
  if (is.numeric(x)) {
    return(str_replace_all(str_replace_all(sprintf("%.2f", x), "[.]", ","), ",00", ""))
  }

  return(x)
}

kable_fr <- function(x, ...) {
  # Alignement selon la type de valeur
  alignment <- ""
  for(i in 1:ncol(x)) {
    if (is.numeric(x[,i])) {
      alignment <- paste(alignment, "r", sep="")
    } else {
      alignment <- paste(alignment, "l", sep="")
    }
  }

  x %>%
    mutate_all(~format_d(.)) %>%
    kable(align=alignment, "pipe", ...)
}

decimal_hook <- function (x) {
  if (is.numeric(x) | is.double(x) | is.integer(x)) {
    # Si un entier, imprimer sans ponctuation decimale; autrement ajouter deux chiffres décimaux
    res <- ifelse(x == round(x),
      sprintf("%d", x),
      str_replace_all(sprintf("%.2f", x), "[.]", ",")
    )
    paste(res, collapse = ", ")
  } else {
    inline_hook_old(x)
  }
}

knit_hooks$set(inline = decimal_hook)
```

```{r}
# Préparation aesthétique

theme_set(theme_classic())

# Échellex de couleur


# Utilisation d'une graine aléatoire déterminée
set.seed(3142)

```

```{r}
# Fonctions générales

# Mode pour les facteurs
Mode <- function(x) {
  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}
```

```{r}
#| echo: FALSE

flat_data <- 
  read.csv("./collected-data/flat-replay-data-5rep.csv", header=TRUE) %>%
  mutate(
    Tag=recode(Tag,
      `0`="Normal",
      `3`="Offroad",
      `5`="Fullspeed",
      `6`="LOL",
      `7`="Tech",
      `8`="SpeedTech",
      `10`="PressForward",
      `12`="Grass",
    )
  )

flat_data_min <- apply(flat_data, 2, min)
flat_data_max <- apply(flat_data, 2, max)

norm2 <- function(x, na.rm = FALSE) (x - min(x, na.rm = na.rm)) / (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))
normalized_flat_data <- flat_data %>% mutate_if(is.numeric, norm2)
```

# Introduction

`{r} 123.4567`

# Méthodes

\lipsum @fig-violin-facet ^[hello] \lipsum

# Résultats

{{< pagebreak >}}

```{r}
#| label: fig-violin-facet
#| echo: FALSE
#| fig-cap: "Les distributions relatives (relatives aux minimums et maximums) des valeurs de chaque variable selon l'étiquette. Les barres rouges représentent l'écart type distancé de la moyenne."
#| fig-width: 8
#| fig-height: 10
#| out-width: '100%'
#| fig-align: 'center'
#| layout-nrow: 1

facet_ordered_colnames <- c("AvgAbsDisplacementHorizontal","AvgAbsDisplacementY","AvgRPM","AvgSteerBias","AvgAbsSteer","AvgSpeedForward","AvgAbsSpeedForward","AvgSpeedSidewardBias","AvgAbsSpeedSideward","AvgSpeedSidewardOppSteer","PercentPitchLowerThird","PercentRollLowerThird","PercentPitchMiddleThird","PercentRollMiddleThird","PercentPitchUpperThird","PercentRollUpperThird","PercentTurbo")

facet_ordered_colours <- c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, "#ffb3b333", "#b3dfff33", "#ffb3b333", "#b3dfff33", "#ffb3b333", "#b3dfff33", NA)

flat_data_long <- gather(flat_data, key="measure", value="value", facet_ordered_colnames)# colnames(flat_data)[!(colnames(flat_data) %in% c("Tag"))])

dfTab <- flat_data_long %>% group_by(Tag, measure) %>%
  summarize(
    mean = mean(value),
    sd = sd(value)
  )

ggplot(flat_data_long, aes(x=Tag, y=value)) + #  %>% group_by(Tag)
  # Diagrammes
  # geom_boxplot() +
  # geom_quasirandom(width = 0.2, alpha = 0.2, size=0.2) +
  geom_violin(
    draw_quantiles = c(0.25, 0.5, 0.75),
    size=0.2,
    colour="black",
    fill = NA,
    scale="width"
  ) + # TODO: Find more representative scale type
  facet_wrap2(
    ~forcats::fct_relevel(measure, facet_ordered_colnames),
    scales="free_y",
    ncol=2,
    strip=strip_themed(
      background_x = elem_list_rect(fill = facet_ordered_colours)
    )
  ) +

  # # Écart type / barres d'érreure
  geom_errorbar(
    data=dfTab,
    aes(x=Tag, y=mean, ymin=mean-sd, ymax=mean+sd),
    width=.2,
    linewidth=.2,
    colour="red"
  ) +
  geom_point(data=dfTab, aes(x=Tag, y=mean), size=0.3, colour="red") +
  # # Anotation de l'écart type
  # geom_text(data=dfTab,aes(x=species,y=y,parse=FALSE,label=sprintf("s = %.2f", dfTab$sd)),vjust=0) +

  # Thèmes
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
    # legend.position = c(0.10, 0.80),
    # legend.background = element_rect(colour = "black")
  ) +

  # Anotations
  labs(
    title = "Distributions relatives de chaque variable",
    x = "Étiquette",
    y = "Valeur"
  )

```

{{< pagebreak >}}

```{python}

import sklearn
import numpy as np
import pandas as pd
import sklearn.metrics
import xgboost as xgb
import shap

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OrdinalEncoder

###################################

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

###################################

# Constants
FILE_NAME = "./collected-data/flat-replay-data-5rep.csv"
SEED = 19842

# load data
full_dataset = pd.read_csv(FILE_NAME) # For Pandas
full_dataset = full_dataset.astype({"Tag": str})

mappingNumToCat = {
  "0": "Normal",
  # "Stunt": 1,
  # "Maze": 2,
  "3": "Offroad",
  # "Laps": 4,
  "5": "Fullspeed",
  "6": "LOL",
  "7": "Tech",
  "8": "SpeedTech",
  # "RPG": 9,
  "10": "PressForward",
  # "Trial": 11,
  "12": "Grass",
}
  
dataset = full_dataset.copy()

dataset['Tag'] = dataset['Tag'].replace(mappingNumToCat) # Map to categorical
  # if removed is not None: dataset = dataset.drop(removed, axis=1)
  # print(dataset.head)

print(dataset)

# split data into X and y
X, y = dataset.drop("Tag", axis=1), dataset[['Tag']] # For Pandas
# X = dataset[:,1:]
# y = dataset[:,0]

# Encode y to numeric
y_encoded = OrdinalEncoder().fit_transform(y)

# split data into train and test sets
# test_size = 0.15

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=SEED, stratify=y_encoded)

# Dummy model #####################################

if True:
  print("\nStart dummy model")
  dummy_model = sklearn.dummy.DummyClassifier()
  dummy_model.fit(X, y_encoded)
  
  y_pred = dummy_model.predict(X_test)
  print(sklearn.metrics.confusion_matrix(y_test, y_pred))
  print(sklearn.metrics.classification_report(y_test, y_pred))
  
  # print(dummy_model.predict(X))
  print("Dummy best %: ", dummy_model.score(X, y_encoded))

# Linear regression model #########################

if True:
  print("\nStart linear regression model")
  from sklearn.linear_model import LogisticRegression
  
  logistic_classifier = LogisticRegression(max_iter=10000, solver="liblinear")
  logistic_classifier.fit(X_train, y_train)
  y_pred = logistic_classifier.predict(X_test)
  print(sklearn.metrics.confusion_matrix(y_test, y_pred))
  print(sklearn.metrics.classification_report(y_test, y_pred))

  count_correct = 0
  for i in range(len(y_test)):
    if (y_pred[i] == y_test[i]):
      count_correct += 1
  percent_correct = count_correct / len(y_test)

  print("Linear Regression %: ", percent_correct)

# XGBoost Model ###################################

if True:
  print("\nStart XGBoost Model")
  # Create classification matrices
  dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)
  dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)

  params = {
    "objective": "multi:softmax",
    "tree_method": "hist",
    "num_class": 8,
    "device": "cuda",
    "max_depth": 5,
    "eval_metric": "mlogloss",
  }
  n_rounds = 100

  results = xgb.cv(
    params, dtrain_clf,
    num_boost_round=n_rounds,
    nfold=2,
    metrics=["mlogloss", "auc", "merror"]
  )

  print("Average test-auc-mean: ", results['test-auc-mean'].sum() / results['test-auc-mean'].count())

  print("Best test-auc-mean: ", results['test-auc-mean'].max())

  # GPU accelerated training
  watchlist = [(dtrain_clf, "train"), (dtest_clf, "validation")]
  evals_result = {}
  model = xgb.train(
    params,
    dtrain_clf,
    n_rounds,
    evals=watchlist,
    evals_result=evals_result,
    early_stopping_rounds=10,
    verbose_eval=10,
  )

  y_pred = model.predict(dtest_clf)

  print(sklearn.metrics.confusion_matrix(y_test, y_pred))
  print(sklearn.metrics.classification_report(y_test, y_pred))

  count_correct = 0
  for i in range(len(y_test)):
    if (y_pred[i] == y_test[i]):
      count_correct += 1
  percent_correct = count_correct / len(y_test)

  print("XGBoost %: ", percent_correct)




  # Compute shap values using GPU with xgboost
  model.set_param({"device": "cuda"})
  shap_values = model.predict(dtrain_clf, pred_contribs=True)

  # Compute shap interaction values using GPU
  shap_interaction_values = model.predict(dtrain_clf, pred_interactions=True)

  # shap will call the GPU accelerated version as long as the device parameter is set to "cuda"
  explainer = shap.TreeExplainer(model)
  shap_values = explainer.shap_values(X)

  # Show a summary of feature importance
  shap.summary_plot(shap_values, features=X, feature_names=X.columns, plot_type="bar", max_display=5)

```

# Discussion



# Conclusion



# Références

::: {#refs}
:::
