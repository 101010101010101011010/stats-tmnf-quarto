---
title: Classification de toute allure!
subtitle: Une analyse de la relation entre l’étiquette et le cheminement de circuits dans Trackmania Nations Forever utilisant la classification
highlight-style: github
authors:
  - name: Nicolas Kmita
    affiliation: École secondaire Franco-Cité
bibliography:
  - references.bib
  - packages.bib
nocite: |
  @*
notebook-links: false
lang: fr
format:
  titlepage-pdf:
    pdfengine: xelatex
    include-in-header:
      - text: |
          \usepackage{xcolor}
          \usepackage{lipsum}
      - macros.tex
    documentclass: scrreprt
    classoption: ["oneside", "open=any"]
    number-sections: true
    toc: true
    toc-title: "Table des matières"
    lof: false
    lot: false
    titlepage: classic-lined 
    titlepage-logo: "img/car-jump.png"
    titlepage-theme:
      elements: ["\\titleblock", "\\authorblock", "\\logoblock", "\\footerblock"]
      page-align: "center"
      title-style: "doublelinewide"
      title-fontsize: 30
      title-fontstyle: "uppercase"
      title-space-after: "0.1\\textheight"
      subtitle-fontstyle: ["Large", "textit"]
      author-style: "plain"
      author-sep: "\\hskip1em"
      author-fontstyle: "Large"
      author-space-after: "2\\baselineskip"
      affiliation-style: "numbered-list-with-correspondence"
      affiliation-fontstyle: "large"
      affiliation-space-after: "0pt"
      footer-style: "plain"
      footer-fontstyle: ["large", "textsc"]
      footer-space-after: "0pt"
      logo-size: "0.7\\textheight"
      logo-space-after: "1cm"
    titlepage-footer: |
      M. Chabot\
      MDM4U\
      25 octobre 2024\
    keep-tex: true
    geometry:
      - showframe
      - inner=2cm
      - outer=2cm
      - top=3cm
      - bottom=4cm
      - headsep=22pt
      - headheight=11pt
      - footskip=33pt
      - ignorehead
      - ignorefoot
      - heightrounded
    indent: false
---

```

```

```{r}

# Préparation de librairies et installations variés

cran_mirror <- "https://mirror.csclub.uwaterloo.ca/CRAN/"

pkgs <- c(
  "tidyverse",
  "ggbeeswarm", # Génère des graphique d'essaim d'abeille
  "viridis", # Couleurs pouvant être mieux perçus
  "kableExtra",
  "rmarkdown",
  "knitr", # Tableaux
  "tinytex",
  "reshape",
  "ggh4x",
  "reticulate"
)

# Charger les « packages » et, s'ils ne sont pas installés, les installer du mirroir de CRAN indiqué ci-haut (UWaterloo par défaut).
for(pkg in pkgs) {
  # Voir https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them

  if(require(pkg, character.only=TRUE)){
    next
  }

  install.packages(pkg, repos=cran_mirror)
  library(pkg, character.only=TRUE)
}
# update.packages(ask = FALSE, repos=cran_mirror)

# Assurer la présence de TinyTeX
if (nchar(tinytex_root()) <= 0) {
  install_tinytex()
}

# Génération de citations
knitr::write_bib(pkgs, prefix="", file="packages.bib", tweak=FALSE)
```

```{r}
# Assurer l'utilisation des virgules par les sorties

inline_hook_old <- knit_hooks$get("inline")

format_d <- function(x) {
  if (is.numeric(x)) {
    return(str_replace_all(str_replace_all(sprintf("%.2f", x), "[.]", ","), ",00", ""))
  }

  return(x)
}

kable_fr <- function(x, ...) {
  # Alignement selon la type de valeur
  alignment <- ""
  for(i in 1:ncol(x)) {
    if (is.numeric(x[,i])) {
      alignment <- paste(alignment, "r", sep="")
    } else {
      alignment <- paste(alignment, "l", sep="")
    }
  }

  x %>%
    mutate_all(~format_d(.)) %>%
    kable(align=alignment, "pipe", ...)
}

decimal_hook <- function (x) {
  if (is.numeric(x) | is.double(x) | is.integer(x)) {
    # Si un entier, imprimer sans ponctuation decimale; autrement ajouter deux chiffres décimaux
    res <- ifelse(x == round(x),
      sprintf("%d", x),
      str_replace_all(sprintf("%.2f", x), "[.]", ",")
    )
    paste(res, collapse = ", ")
  } else {
    inline_hook_old(x)
  }
}

knit_hooks$set(inline = decimal_hook)
```

```{r}
# Préparation aesthétique

theme_set(theme_classic())

# Échellex de couleur


# Utilisation d'une graine aléatoire déterminée
set.seed(3142)

```

```{r}
# Fonctions générales

# Mode pour les facteurs
Mode <- function(x) {
  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}
```

```{r}
#| echo: FALSE

flat_data <- 
  read.csv("./collected-data/flat-replay-data-5rep.csv", header=TRUE) %>%
  mutate(
    Tag=recode(Tag,
      `0`="Normal",
      `3`="Offroad",
      `5`="Fullspeed",
      `6`="LOL",
      `7`="Tech",
      `8`="SpeedTech",
      `10`="PressForward",
      `12`="Grass",
    )
  )

flat_data_min <- apply(flat_data, 2, min)
flat_data_max <- apply(flat_data, 2, max)

norm2 <- function(x, na.rm = FALSE) (x - min(x, na.rm = na.rm)) / (max(x, na.rm = na.rm) - min(x, na.rm = na.rm))
normalized_flat_data <- flat_data %>% mutate_if(is.numeric, norm2)
```

# Introduction

```{r}
#| label: fig-violin-facet
#| echo: FALSE
#| fig-cap: "Les distributions relatives (relatives aux minimums et maximums) des valeurs de chaque variable selon l'étiquette. Les barres rouges représentent l'écart type distancé de la moyenne."
#| fig-width: 8
#| fig-height: 10
#| out-width: '100%'
#| fig-align: 'center'
#| layout-nrow: 1

facet_ordered_colnames <- c("AvgAbsDisplacementHorizontal","AvgAbsDisplacementY","AvgRPM","AvgSteerBias","AvgAbsSteer","AvgSpeedForward","AvgAbsSpeedForward","AvgSpeedSidewardBias","AvgAbsSpeedSideward","AvgSpeedSidewardOppSteer","PercentPitchLowerThird","PercentRollLowerThird","PercentPitchMiddleThird","PercentRollMiddleThird","PercentPitchUpperThird","PercentRollUpperThird","PercentTurbo")

facet_ordered_colours <- c(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, "#ffb3b333", "#b3dfff33", "#ffb3b333", "#b3dfff33", "#ffb3b333", "#b3dfff33", NA)

flat_data_long <- gather(flat_data, key="measure", value="value", facet_ordered_colnames)# colnames(flat_data)[!(colnames(flat_data) %in% c("Tag"))])

dfTab <- flat_data_long %>% group_by(Tag, measure) %>%
  summarize(
    mean = mean(value),
    sd = sd(value)
  )

ggplot(flat_data_long, aes(x=Tag, y=value)) + #  %>% group_by(Tag)
  # Diagrammes
  # geom_boxplot() +
  # geom_quasirandom(width = 0.2, alpha = 0.2, size=0.2) +
  geom_violin(
    draw_quantiles = c(0.25, 0.5, 0.75),
    size=0.2,
    colour="black",
    fill = NA,
    scale="width"
  ) + # TODO: Find more representative scale type
  facet_wrap2(
    ~forcats::fct_relevel(measure, facet_ordered_colnames),
    scales="free_y",
    ncol=2,
    strip=strip_themed(
      background_x = elem_list_rect(fill = facet_ordered_colours)
    )
  ) +

  # # Écart type / barres d'érreure
  geom_errorbar(
    data=dfTab,
    aes(x=Tag, y=mean, ymin=mean-sd, ymax=mean+sd),
    width=.2,
    linewidth=.2,
    colour="red"
  ) +
  geom_point(data=dfTab, aes(x=Tag, y=mean), size=0.3, colour="red") +
  # # Anotation de l'écart type
  # geom_text(data=dfTab,aes(x=species,y=y,parse=FALSE,label=sprintf("s = %.2f", dfTab$sd)),vjust=0) +

  # Thèmes
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
    # legend.position = c(0.10, 0.80),
    # legend.background = element_rect(colour = "black")
  ) +

  # Anotations
  labs(
    title = "Distributions relatives de chaque variable",
    x = "Étiquette",
    y = "Valeur"
  )

```

# Méthodes



# Résultats

```{python}
#| label: fig-model-stats
#| fig-cap: "Les matrices de confusion des trois modèles statistiques utilisés: un simulacre simple, une régression logistique, et un amplification de gradients (XGBoost)."
#| out-width: '100%'
#| fig-align: 'center'

import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib import colormaps

import sklearn
import pandas as pd
import sklearn.metrics
import shap
import xgboost as xgb
import joblib

from sklearn.metrics import accuracy_score
from sklearn.preprocessing import OrdinalEncoder

from staty_base import stratified_train_test_split

def add_confusion_matrix_plot(axs, real, pred, name: str, col, accuracy: float):
  ax = axs[col-1]

  cmd = sklearn.metrics.ConfusionMatrixDisplay.from_predictions(
    y_true=real,
    y_pred=pred,
    display_labels=y['Tag'].drop_duplicates(),
    ax=ax,
    colorbar=False,
  )
  cmd.ax_.tick_params(axis='x', labelrotation=75)
  cmd.ax_.set(
    xlabel=None,
    ylabel=None,
    title=name + f" ({accuracy * 100:.2f} %)".replace(".", ",")
  )

FILE_NAME = "./collected-data/flat-replay-data-5rep.csv"
MODEL_STORARE_DIR = "./models/"
SEED = 3142

dataset = pd.read_csv(FILE_NAME) # For Pandas
dataset = dataset.astype({"Tag": str})
mappingNumToCat = {
  "0": "Normal",
  # "Stunt": 1,
  # "Maze": 2,
  "3": "Offroad",
  # "Laps": 4,
  "5": "Fullspeed",
  "6": "LOL",
  "7": "Tech",
  "8": "SpeedTech",
  # "RPG": 9,
  "10": "PressForward",
  # "Trial": 11,
  "12": "Grass",
}
dataset['Tag'] = dataset['Tag'].replace(mappingNumToCat) # Map to categorical
# split data into X and y
X, y = dataset.drop("Tag", axis=1), dataset[['Tag']] # For Pandas
# Encode y to numeric
y_encoded = OrdinalEncoder().fit_transform(y)
# Split the data
X_train, y_train, X_test, y_test = stratified_train_test_split(y_encoded, X)

fig, axs = plt.subplots(figsize=(2*5, 1*5), ncols=3, nrows=1, sharex=True, sharey=True, layout="constrained")

if True:
  model = joblib.load(MODEL_STORARE_DIR + "dummy_model.pkl")
  y_pred = model.predict(X_test)
  
  add_confusion_matrix_plot(
    axs=axs,
    real=y_test,
    pred=y_pred,
    name="Simulacre",
    col=1,
    accuracy=model.score(X, y_encoded)
  )

if True:
  model = joblib.load(MODEL_STORARE_DIR + "logistic_regression_model.pkl")
  y_pred = model.predict(X_test)

  count_correct = 0
  for i in range(len(y_test)):
    if (y_pred[i] == y_test[i]):
      count_correct += 1
  percent_correct = count_correct / len(y_test)
  
  add_confusion_matrix_plot(
    axs=axs,
    real=y_test,
    pred=y_pred,
    name="Régression logistique",
    col=2,
    accuracy=percent_correct
  )

if True:
  dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical=True)
  
  model = joblib.load(MODEL_STORARE_DIR + "xgboost_model.pkl")
  y_pred = model.predict(dtest_clf)
  
  count_correct = 0
  for i in range(len(y_test)):
    if (y_pred[i] == y_test[i]):
      count_correct += 1
  percent_correct = count_correct / len(y_test)

  add_confusion_matrix_plot(
    axs=axs,
    real=y_test,
    pred=y_pred,
    name="XGBoost",
    col=3,
    accuracy=percent_correct
  )


plt.suptitle("Matrices de confusion")

count_max = y_test[y_test[:] == y_test[0]].shape[0]
cbar_ax = fig.add_axes([0.05, 0.01, 0.02, 0.22])
colorbar = fig.colorbar(
  mpl.cm.ScalarMappable(
    norm=mpl.colors.Normalize(
      vmin=0,
      vmax=count_max
    ),
    cmap=colormaps.get_cmap("viridis")
  ),
  cax=cbar_ax,
  orientation='vertical'
)
colorbar.set_ticks([0, count_max])

# plt.tight_layout()
# plt.subplots_adjust(bottom=0.25, left=0.15)
fig.supylabel("Réel")
fig.supxlabel("Prédite")
plt.show()
```

```{python}
dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical=True)
xgboost_model = joblib.load(MODEL_STORARE_DIR + "xgboost_model.pkl")

# Compute shap values using GPU with xgboost
xgboost_model.set_param({"device": "cuda"})
shap_values = xgboost_model.predict(dtrain_clf, pred_contribs=True)

# Compute shap interaction values using GPU
shap_interaction_values = xgboost_model.predict(dtrain_clf, pred_interactions=True)

# shap will call the GPU accelerated version as long as the device parameter is set to "cuda"
explainer = shap.TreeExplainer(xgboost_model)
shap_values = explainer.shap_values(X)

# Show a summary of feature importance
# shap.summary_plot(shap_values, features=X, feature_names=X.columns, plot_type="bar", max_display=5)
shap.summary_plot(shap_values, features=X, feature_names=X.columns, plot_type="bar")
```

# Discussion



# Conclusion



# Références

::: {#refs}
:::
